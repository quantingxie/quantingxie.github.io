---
title: "MM-SeR: Multimodal Self-Refinement for Lightweight Image Captioning"
category: manuscripts
permalink: /publication/2025-mm-ser
date: 2025-08-29
venue: "arXiv preprint"
authors: "Junha Song, Yongsik Jo, So Yeon Min, Quanting Xie, Taehwan Kim, Yonatan Bisk, Jaegul Choo"
selected: false
paperurl: "https://arxiv.org/abs/2508.21451"
weburl: "https://sites.google.com/view/junha/mm-ser"
excerpt: "A lightweight captioning model using multimodal self-refinement."
header:
    teaser: MM-SeR.png
---

**Authors:** Junha Song, Yongsik Jo, So Yeon Min, Quanting Xie, Taehwan Kim, Yonatan Bisk, Jaegul Choo

## Abstract

Systems such as video chatbots and navigation robots often depend on streaming image captioning to interpret visual inputs. Existing approaches typically employ large multimodal language models (MLLMs) for this purpose, but their substantial computational cost hinders practical application. This limitation motivates our development of a lightweight captioning model. Our investigation begins by replacing the large-scale language component in MLLMs with a compact 125M-parameter model. Surprisingly, this compact model, despite a 93x reduction in size, achieves comparable performance to MLLMs, suggesting that factual image captioning does not significantly require the complex reasoning abilities of LLMs. Despite this promising result, our lightweight model still lacks reliability. To address this, we draw inspiration from the human visual process: perceiving a global and coarse understanding of the scene before attending to finer details. Accordingly, we propose a multimodal self-refinement framework that guides the model to utilize features from salient regions, identified by referencing the previous coarse caption, and to produce a refined description. Experimental results demonstrate the superiority of our model in both single-sentence and detailed captioning, extending even to long-range video QA tasks.

## Links

- Paper: `https://arxiv.org/abs/2508.21451`
- Project: `https://sites.google.com/view/junha/mm-ser`

